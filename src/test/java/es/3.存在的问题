------------------ 脑裂现象
    多个master存在
    https://blog.csdn.net/nphyez/article/details/89161417
------------------ 扩容[扩容后每个节点上的分片更少[均匀分配]，每个分片可以利用到更多的资源]
    如何超出系统扩容瓶颈，比如6个分片但是要扩容到9台机器，主分片不能改变，可以增加分片副本

------------------ 分页或者聚合效率可能很低[将不同的数据加载到内存中]

------------------ 并发问题
    写入数据不一致
    解决方案：
        悲观锁：在各种情况下都加锁，只允许一个线程操作数据
        乐观锁：[es使用(数据版本号)]乐观锁是不加锁的--写的时候会判断当前数据的版本号是否跟es中的数据版本号是否一致
            [一致才能修改修改后版本号发生变化，不一致就重新读取数据再次操作(类似与自旋)]es中对数据的修改删除版本号都会+1

    es的备份分片的数据同步问题[向备份分片发送请求都是异步的，同步数据的顺序可能会发生变化]
        解决：基于版本号 同步过来的数据会带着版本号假如版本号一致会做同步，假如不一致不会做同步[不会让就数据覆盖新数据]

    模拟并发修改[es基于乐观锁使用版本号进行控制]
        PUT /bingfa/bf/1?version=1
        {
          "name":"Alice"
        }
        报错信息：
            "reason": "[bf][1]: version conflict, current version [2] is different than the one provided [1]",

        使用外部的版本号：
            PUT /bingfa/bf/1?version=3&version_type=external
            {
              "nanme":"Alice"
            }
            假如版本号一致或者更小
            "reason": "[bf][1]: version conflict, current version [3] is higher or equal to the one provided [3]",
        retry策略：尝试指定次数
            post /index/type/id/_update?retry_on_conflic=5&version

========================================== 集群下分页深度问题 ====================================================
在集群系统中深度分页
    数量小时：
        为了理解为什么深度分页是有问题的，让我们假设在一个有5个主分片的索引中搜索。当我们请求结果的第一页（结果1到10）时，每个分片产生自己最顶端10个结果然后返回它们给请求节点(requestingnode)，它再排序这所有的50个结果以选出顶端的10个结果。
    数量大时：
        现在假设我们请求第1000页——结果10001到10010。工作方式都相同，不同的是每个分片都必须产生顶端的10010个结果。然后请求节点排序这50050个结果并丢弃50040个！
    你可以看到在分布式系统中，排序结果的花费随着分页的深入而成倍增长。这也是为什么网络搜索引擎中任何语句不能返回多于1000个结果的原因。

========================================== 倒排索引存在的问题 ====================================================
单复数和同义词没法匹配eg:"Quick" 和 "quick" ;"fox" 和 "foxes";"jumped" 和 "leap"
方案1：将词为统一为标准格式，这样就可以找到不是确切匹配查询，但是足以相似从而可以关联的文档。eg:. "Quick"-->"quick" ;"foxes"-->"fox";"jumped"和"lip"就可以只索引"jump"