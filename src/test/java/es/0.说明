图解: https://blog.csdn.net/zl1zl2zl3/article/details/84799928


======================================= 说明:=======================================
http://localhost:9200/
{
  "name" : "pP2wAzl",//node名称
  "cluster_name" : "elasticsearch_xxm",  //集群名称
  "cluster_uuid" : "6s7UFSQQTwaTxekm5e3KVg",
  "version" : {
    "number" : "6.7.0", //版本号
    "build_flavor" : "oss",
    "build_type" : "tar",
    "build_hash" : "8453f77",
    "build_date" : "2019-03-21T15:32:29.844721Z",
    "build_snapshot" : false,
    "lucene_version" : "7.7.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
配置文件：elasticsearch.yml
======================================= elastic特点:=======================================

    [NRT]近实时:1-从写入数据到可以被搜索有一个延迟(大概1s)  ;2-基于es搜索和分析可以达到秒级
    集群节点[Cluster,Node]:
    shard[每个索引默认10个shard,5个primary shard(建立索引时一次设置默认5个不能修改),5个replica shard]  es规定shard与replica必须不在一个节点上[一般集群最少两台机器]
        shard[primary shrd][lunce index]:index[shard(分布在多个节点上-吞吐量横向扩展)] 参考图解
            假设一台机器的吞吐量是2000 shard在不同的node上 node又分布在不同的机器上 就达到了横向扩展的目的
        replica[replica shard] 确保每个shard都有副本信息replica--防止宕机数据丢失
            高可用:防止宕机数据丢失
            提升搜索操作的吞吐量和性能:提高每秒查询率[QPS] 查询shard与replica
    与mysql对比:
        index       库
        type        表
        document    行
    面向文档的数据结构:无论一个文档嵌套多少层 在es中是以一个documennt来表示 而不像mysql用关联关系来表示
------------------倒排索引(Inverted Index)
每一个文档都对应一个ID。倒排索引会按照指定语法对每一个文档进行分词，然后维护一张表，列举所有文档中出现的terms以及它们出现的文档ID和出现频率。
    搜索时同样会对关键词进行同样的分词分析，然后查表得到结果。
ES对于JSON文档中的每一个field都会构建一个对应的倒排索引

------------------增加文件可能会使索引空间变小[lunce擅长压缩]
    随着时间的增加，我们会有很多segments，所以ElasticSearch会将这些segment合并，在这个过程中，segment会最终被删除掉,
    这就是为什么增加文件可能会使索引所占空间变小，它会引起merge，从而可能会有更多的压缩

------------------路由        https://blog.csdn.net/weixin_39471249/article/details/80600176
这个请求可能被分发到集群里的任意一个节点
每个节点，每个都存留一份路由表，所以当请求到任何一个节点时，ElasticSearch都有能力将请求转发到期望节点的shard进一步处理。
存储的时候 根据routing[一般是文档id也可以自己设置] 和hash函数 shard = hash(routing) % number_of_primary_shards 再根据分片总数计算出 存储位置[哪一个分片上]
查找的时候 更具routing计算出位置

------------------ es缓存
现象:第一次搜索5-10s后面搜索变快
性能优化:Filesystem Cache
写数据的时候是写到磁盘里,查询的时候,操作系统会将磁盘文件里的数据自动缓存到Filesystem Cache里
Es的搜索引擎严重依赖于 Filesystem Cache 如果将这个内存设置的很大 容纳更多的IDX Segement File 搜索的时候基本都是走内存的 性能会很高

如果内存留给Es的过小-超出的部分会走磁盘查询-性能下降
最佳:在Es中存少量的数据[仅用来查询的数据],其他数据放在其他地方[Hbase]

------------------ 冷热分离
将冷数据放在一个索引 [查询少,效率低无所谓]
将热数据放在一个索引 [频繁的查询走Filesystem Cache]性能高

------------------  Document设计
合理的设计
关联查询性能都很差   eg:join/nested/parent-child
------------------
